In lab notes 1, I explored the complete transformer architectures, both encoders and decoders... from this, I was able to load in progen2-oas, as well as a guide on fine-tuning the model. 
From Hugo Hrbáň's bachelor thesis, I was able to extract the use of the model by feeding it random AI-generated sequences to generate the characters that are likely. 
From this, I plotted the spread of the characters to get a sense of how the occurences were distributed. 

# The folder "external" has the external resources that I used to load and get familar with fasta data types, as well as the architecture of the model.

# The folder "lab_testing" has my own experience in testing out the model, and generating sequences given a certain input.